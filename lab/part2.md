### Prerequisites

Before beggining this portion it is important to have a wandb account. Use [this link](https://docs.wandb.ai/quickstart), to set one up. 
### Training the gym environment using an RL framework (TorchRL)

Now that we have shown how an gym environment is constructed, we need to train an RL agent to interact with the environment. There are many existing RL frameworks that are compatible with the standard gym environment, here are some notable examples:

* [TorchRL](https://pytorch.org/rl/), torchRL is part of the famous pytorch framework, development lead by Vincent Moens from Meta.
* [RLLib/Ray](https://docs.ray.io/en/latest/rllib/index.html), RLLib/Ray is an open-source RL library for distributed workloads, useful with multi-agent setting. 
* [Rlmeta](https://github.com/facebookresearch/rlmeta), also developed by Meta.

These frameworks have their individual strengths. However, for the cache guessing game, all of them works for this purpose. We use TorchRL as the example. For TorchRL, the related torchRL script are located in ```src/torchrl/```. For RLMeta, the related script are located in ```src/rlmeta/```. 

### Train the RL agent using TorchRL

[```src/torchrl/train_ppo_attack.py```](https://github.com/ut-ldma/lab/blob/main/src/torchrl/train_ppo_attack.py) is the script for training. The default configuration for training is specified in [```src/torchrl/config/ppo_attack.yaml```](https://github.com/ut-ldma/lab/blob/main/src/torchrl/config/ppo_attack.yaml). The environment configuration is specified in [```src/torchrl/config/env_config/```](https://github.com/ut-ldma/lab/blob/main/src/torchrl/config/env_config) and the model configuration of the policy net is specified in 
[```src/torchrl/config/model_config/```](https://github.com/ut-ldma/lab/blob/main/src/torchrl/config/model_config).

Please use the following to launch the training

```
python /lab/src/rlmeta/train_ppo_attack.py env_config=hpca_ae_exp_4_1
```

select (2) when prompted to plot all the training progress in wandb. 

On your browser, open a new tab and open ```https://wandb.ai``` and navigate to the ```rl4cache``` project, where you can see the current training and testing return and other statistics. As the training goes, the return will gradually increase to a positive value close to 1.0, which indicates that the RL agent (attacker) will be able to guess the secret with high accuracy. (Since the correct guess reward is 1.0 and wrong guess reward is -1.0, only if most of the guesses are correct will the reward be close to 1.0). The script will also automatically save the checkpoint of the model of the policy net in ```src/torchrl/output``` directory.

### Sampling the policy net to get the attack trajectory using TorchRL

Once we have trained the policy net to have very high guess correct rate, we want to get a sample of the attack trajectory generated by this policy net, this can be done using the following command.

```
python /AutoCAT/src/torchrl/sampling_trajectories.py --saved_path=saved_ppo_attack-exp0 --num-rollouts=1
```
Which it will generate the attack sequence.








