### Train the RL agent using RLMeta

```src/rlmeta/train_ppo_attack.py``` is the script for training. The default configuration for training is specified in ```src/rlmeta/config/ppo_attack.yml```. The environment configuration is specified in ```src/torchrl/config/env_config/``` and the model configuration of the policy net is specified in 
```src/rlmeta/config/model_config/```.

RLMeta script assumes ```gym==0.26```, please update the gym version in the ```py38``` conda env.

```
pip install gym==0.26
```

Please use the following to launch the training

```
python ${REPO_ROOT}/src/rlmeta/train_ppo_attack.py env_config=<PATH_TO_ENV_CONFIG>
```

Currently RLMeta script is not integrated with Wandb, the training progress is plotted directly on the terminal.

### Sampling the policy net to get the attack trajectory using RLMeta

Once we have trained the policy net to have very high guess correct rate, we want to get a sample of the attack trajectory generated by this policy net, this can be done using the following command.

```
python ${REPO_ROOT}/src/rlmeta/sample_attack.py ${PATH_TO_CHECKPOINT}
```
Which it will generate the attack sequence.
